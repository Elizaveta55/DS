# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qi9FdrQisyXMy266hIzCHEoFsj2oWvdg
"""

import torch
from torch import nn

import torchtext
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import Vocab
from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import DataLoader, TensorDataset

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

import pandas as pd

train_data = pd.read_csv("./train_eng.csv", header=0, engine='python' ,encoding = "latin-1", usecols=["Name","Gender"])
test_data = pd.read_csv("./test_eng.csv", header=0, engine='python' ,encoding = "latin-1", usecols=["Name","Gender"])

test_data['Gender'] = test_data['Gender'].apply(lambda x: 0 if x=='M' else 1)
train_data['Gender'] = train_data['Gender'].apply(lambda x: 0 if x=='M' else 1)


#print(train_data.shape)
#train_data = train_data[:1280]
#test_data = test_data[:1280]
#print(train_data.shape)

train_data = train_data.sort_values(by="Name", key=lambda x: x.str.len())
test_data = test_data.sort_values(by="Name", key=lambda x: x.str.len())

max_length_test = len(test_data.iloc[-1]['Name'])
max_length_train = len(train_data.iloc[-1]['Name'])
max_length = max(max_length_test, max_length_train)

unique = list(set("".join(train_data.iloc[:,0])))
unique.sort()
vocab = dict(zip(unique, range(1,len(unique)+1)))

tokenizer = get_tokenizer('basic_english')

vocab_new = Vocab(vocab,specials=())

def data_process(raw_text_iter,max_len=128):
  batch = []
  for item in raw_text_iter:
    res = []
    for i in range(max_len):
      if (len(item)>i):
        res.extend([vocab_new[token] for token in tokenizer(item[i])])
      else:
        res.extend([0])
    batch.append(res)
  pad_data = torch.FloatTensor(batch)
  return pad_data

import numpy as np
from torch.utils.data import DataLoader, TensorDataset
scaler = MinMaxScaler(feature_range=(-1, 1))

max_len = 64
embedding_size = max(max_length_train, max_length_test)
n_classes = len(np.unique(train_data.Gender.values))

#Create Dataloader
train_tensor = data_process(train_data.Name.values, embedding_size)
train_data_normalized = torch.FloatTensor(scaler.fit_transform(train_tensor))
print(train_data_normalized)
tgts_tensor = torch.nn.functional.one_hot(torch.from_numpy(train_data.Gender.values), n_classes) #torch.from_numpy(train_data.Target.values)

dataset = TensorDataset(train_data_normalized, tgts_tensor)

loader = DataLoader(dataset, batch_size=128, shuffle=True, pin_memory=True)

print(train_data_normalized)
print()
print(tgts_tensor)
print()
print(torch.tensor(train_data_normalized, dtype=torch.double))

class LSTM(torch.nn.Module):
  def __init__(self, input_size=max_length, hidden_layer_size=100, output_size=2, vocab_size=52, embedding_dim = max_length):
        super().__init__()
        self.hidden_layer_size = hidden_layer_size
        #self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(input_size, hidden_layer_size)
        self.linear = nn.Linear(hidden_layer_size, output_size)
        self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),
                            torch.zeros(1,1,self.hidden_layer_size))

  def forward(self, input_seq):
        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq) ,1, -1), self.hidden_cell)
        predictions = self.linear(lstm_out.view(len(input_seq), -1))
        return predictions

from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir runs

import time 
import torch.nn.functional as F
start_time = time.time()

lrs =  [0.001]
max_epochs =  [50]
loss_functions = [nn.BCEWithLogitsLoss()]
n_layers = [200]

for epochs in max_epochs:
  for lr in lrs:
      for loss_function in loss_functions:
        for hidden_layer_size in n_layers:
          model = LSTM(hidden_layer_size=hidden_layer_size, embedding_dim = max_length)
          optimizers = [torch.optim.Adam(model.parameters(), lr)]
          for optimizer in optimizers:
            print("MODEL")
            #print("__________________________________________________________")
            #print("Parameters:")
            #print("Amount of epochs", epochs)
            #print("Learning rate", lr)
            #print("Loss function", loss_function)
            #print("Hidden layer size", hidden_layer_size)
            #print("Optimizer", optimizer)
            for i in range(epochs):
                correct = 0
                total = 0
                start_epoch = time.time()
                for item in loader:
                    seq = item[0]
                    label = item[1]
                    optimizer.zero_grad()
                    model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),
                                    torch.zeros(1, 1, model.hidden_layer_size))

                    y_pred = model(seq)
                    for j in range(y_pred.shape[0]):
                      predicted = np.argmax(F.softmax(y_pred[j].data))
                      true = np.argmax(label[j].data)
                      if (predicted == true):
                        correct+=1
                      total+=1
                    single_loss = loss_function(y_pred, label.float())
                    single_loss.backward()
                    optimizer.step()

                time_per_epoch = time.time() - start_epoch
                f_measure = f1_loss(label, y_pred)
                writer.add_scalar("train_loss", single_loss.item(), i)
                writer.add_scalar("train_acc", (100 * correct / total), i)
                writer.add_scalar("train_measure", f_measure, i)
                writer.add_scalar("train_time", time_per_epoch, i)
                for tag, parm in model.named_parameters():
                  writer.add_histogram(tag, parm.grad.data.cpu().numpy(), i)
                print(f'epoch: {i:3} loss: {single_loss.item():10.8f}, accuracy: {(100 * correct / total)}, f-measure: {f_measure}, time = {time_per_epoch}')

            print("MODEL TIME EXECUTION--- %s seconds ---" % (time.time() - start_time))
            print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')
            print()
            print()

def f1_loss(y_true:torch.Tensor, y_pred:torch.Tensor, is_training=False) -> torch.Tensor:
    all = y_pred.shape[0]
    sum=0
    for i in range(all):
      sum+=f1_loss_one(y_true[i], y_pred[i])
    return sum/all

def f1_loss_one(y_true:torch.Tensor, y_pred:torch.Tensor, is_training=False) -> torch.Tensor:
    y_pred_soft = (F.softmax(((y_pred)), dim=0)).detach()
    if y_pred_soft.ndim == 2:
        y_pred_soft = y_pred_soft.argmax(dim=1)
    if y_true.ndim == 2:
        y_true = y_true.argmax(dim=1)
    
    tp = (y_true * y_pred_soft).sum().to(torch.float32)
    tn = ((1 - y_true) * (1 - y_pred_soft)).sum().to(torch.float32)
    fp = ((1 - y_true) * y_pred_soft).sum().to(torch.float32)
    fn = (y_true * (1 - y_pred_soft)).sum().to(torch.float32)
    
    epsilon = 1e-7
    
    precision = tp / (tp + fp + epsilon)
    recall = tp / (tp + fn + epsilon)
    
    f1 = 2* (precision*recall) / (precision + recall + epsilon)
    #f1.requires_grad = is_training
    return f1

print(test_data)
scaler = MinMaxScaler(feature_range=(-1, 1))

max_len = 64
embedding_size = max(max_length_train, max_length_test)
n_classes = len(np.unique(test_data.Gender.values))

#Create Dataloader
test_tensor = data_process(test_data.Name.values, embedding_size)
test_data_normalized = torch.FloatTensor(scaler.fit_transform(test_tensor))
print(test_data_normalized)
test_tgts_tensor = torch.nn.functional.one_hot(torch.from_numpy(test_data.Gender.values), n_classes) #torch.from_numpy(train_data.Target.values)

test_dataset = TensorDataset(test_data_normalized, test_tgts_tensor)

loader = DataLoader(dataset, batch_size=128, shuffle=True, pin_memory=True)

print(test_data_normalized)
print()
print(test_tgts_tensor)

torch.save(model.state_dict(), 'model.pt')

def evaluate_model(model, data_batches, loss_function):
  eval_loss = 0
  eval_acc = 0
  
  model.eval()
  
  with torch.no_grad():
    for batch in data_batches:
      correct = 0
      total = 0
      predictions = model(batch[0])
      for j in range(predictions.shape[0]):
          predicted = np.argmax(F.softmax(predictions[j].data))
          true = np.argmax(batch[1][j].data)
          if (predicted == true):
            correct+=1
          total+=1
      loss = loss_function(predictions, batch[1].float())
      
      #acc = accuracy_calculator(predictions, batch[1])
      eval_loss += loss.item()
      eval_acc += (correct / total)
  
  return eval_loss / len(data_batches), eval_acc / len(data_batches)

model.load_state_dict(torch.load('model.pt'))
loss = nn.MSELoss()
test_loss, test_acc = evaluate_model(model, loader, loss_functions[0])
print(f'Accuracy on test data : {test_acc*100:.2f}%')