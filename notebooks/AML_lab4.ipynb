{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lab4&HW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elizaveta55/DS/blob/master/lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKvVwL8cBn16"
      },
      "source": [
        "### Week 4: Reccurent Neural Networks (LSTM, GRU, ATTENTION, Transformer, BERT)\n",
        "\n",
        "```\n",
        "- Advanced Machine Learning, Innopolis University \n",
        "- Professor: Muhammad Fahim \n",
        "- Teaching Assistant: Gcinizwe Dlamini\n",
        "```\n",
        "<hr>\n",
        "\n",
        "\n",
        "```\n",
        "Lab Plan\n",
        "1. Simple and staked LSTM\n",
        "2. Transformer based models\n",
        "3. Homework 1 presentation\n",
        "4. Lab Task\n",
        "```\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSWjYWxHRVNa"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Preliminaries for processing the text\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import Vocab\n",
        "import torchtext\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx8uw8WQRx1K"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i0XH-2jyX-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53322dac-45aa-4e32-9f2e-b13440e1f374"
      },
      "source": [
        "!pip install wget \n",
        "import wget "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9681 sha256=2e920ef929348bd50d75f78a6d6cb5a078fe69035df9f07f01a7c4cde3e13fd3\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdM4OJi4LeRz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "969c8e5f-375d-418d-ac08-1f1161fc9b32"
      },
      "source": [
        "#Download and unzip dataset\n",
        "wget.download(\"http://alt.qcri.org/semeval2016/task6/data/uploads/stancedataset.zip\")\n",
        "\n",
        "!unzip stancedataset.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  stancedataset.zip\n",
            "   creating: StanceDataset/\n",
            "  inflating: StanceDataset/test.csv  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/StanceDataset/\n",
            "  inflating: __MACOSX/StanceDataset/._test.csv  \n",
            "  inflating: StanceDataset/train.csv  \n",
            "  inflating: __MACOSX/StanceDataset/._train.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvM_MHehRwGK"
      },
      "source": [
        "#Read dataset to dataframe\n",
        "\n",
        "train_data = pd.read_csv(\"./StanceDataset/train.csv\", header=0, engine='python' ,encoding = \"latin-1\", usecols=[\"Tweet\",\"Target\"])\n",
        "test_data = pd.read_csv(\"./StanceDataset/test.csv\", header=0, engine='python' ,encoding = \"latin-1\", usecols=[\"Tweet\",\"Target\"])\n",
        "\n",
        "test_data.query(\"Target != 'Donald Trump'\",inplace=True)\n",
        "\n",
        "labels_keys = {value: i for i, (value, count) in enumerate(train_data.Target.value_counts().items())}\n",
        "\n",
        "train_data['Target'] = train_data['Target'].apply(lambda x: labels_keys.get(x))\n",
        "test_data['Target'] = test_data['Target'].apply(lambda x: labels_keys.get(x))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMtg69tokX1H",
        "outputId": "ff23426c-258b-49ed-fa51-e8500e3c6194"
      },
      "source": [
        "print(train_data)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                  Tweet  Target\n",
            "0     @tedcruz And, #HandOverTheServer she wiped cle...       0\n",
            "1     Hillary is our best choice if we truly want to...       0\n",
            "2     @TheView I think our country is ready for a fe...       0\n",
            "3     I just gave an unhealthy amount of my hard-ear...       0\n",
            "4     @PortiaABoulger Thank you for adding me to you...       0\n",
            "...                                                 ...     ...\n",
            "2909  There's a law protecting unborn eagles, but no...       2\n",
            "2910  I am 1 in 3... I have had an abortion #Abortio...       2\n",
            "2911  How dare you say my sexual preference is a cho...       2\n",
            "2912  Equal rights for those 'born that way', no rig...       2\n",
            "2913  #POTUS seals his legacy w/ 1/2 doz wins. The #...       2\n",
            "\n",
            "[2914 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niNoZ2dRDh8w"
      },
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(sen):\n",
        "  sentence = remove_tags(sen)\n",
        "  sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "  sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "  sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "  return sentence\n",
        "\n",
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def remove_tags(text):\n",
        "  return TAG_RE.sub('', text)\n",
        "\n",
        "#TODO: preprocess each an every sentence (tweet text)\n",
        "def clean_data(text):\n",
        "  X = []\n",
        "  sentences = list(text)\n",
        "  for sen in sentences:\n",
        "    if sen is not None:\n",
        "      X.append(preprocess_text(sen))\n",
        "  return X\n",
        "\n",
        "#train_data['Tweet'] = train_data['Tweet'].apply(clean_data)\n",
        "#test_data['Tweet'] = test_data['Tweet'].apply(clean_data)\n",
        "\n",
        "train_data['Tweet'] = clean_data(train_data['Tweet'])\n",
        "test_data['Tweet'] = clean_data(test_data['Tweet'])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAMIafLjCryV"
      },
      "source": [
        "#tokenize and create Vocab\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "counter = Counter()\n",
        "\n",
        "for _, row in train_data.iterrows():\n",
        "  counter.update(tokenizer(row[\"Tweet\"]))\n",
        "\n",
        "vocab = Vocab(counter,specials=(\"<pad>\",\"<unk>\"), min_freq=1)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kLLyQvVJpWu"
      },
      "source": [
        "# Do padding\n",
        "def data_process(raw_text_iter,max_len=64):\n",
        "  batch = []\n",
        "  for item in raw_text_iter:\n",
        "    res = [vocab[token] for token in tokenizer(item)]\n",
        "    if len(res) > max_len : \n",
        "      res = res[:max_len]\n",
        "    if len(res) < max_len :\n",
        "      res += ([vocab[\"<pad>\"]] * (max_len-len(res)))\n",
        "    batch.append(res)\n",
        "  pad_data = torch.tensor(batch, dtype=torch.long)\n",
        "  return pad_data"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBGXz8-4kq2t"
      },
      "source": [
        "max_len = 64\n",
        "embedding_size = 10\n",
        "n_classes = len(np.unique(train_data.Target.values))\n",
        "\n",
        "#Create Dataloader\n",
        "train_tensor = data_process(train_data.Tweet.values)\n",
        "tgts_tensor = torch.nn.functional.one_hot(torch.from_numpy(train_data.Target.values), n_classes) #torch.from_numpy(train_data.Target.values)\n",
        "\n",
        "dataset = TensorDataset(train_tensor, tgts_tensor)\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=2, shuffle=True, pin_memory=True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP63bpU_nks6"
      },
      "source": [
        "## Simple LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_splgvZEjfHi"
      },
      "source": [
        "class SimpleLstm(nn.Module):\n",
        "  def __init__(self, embedding_dim ,vocab_size , hidden_dim=10, output_dim=1, n_layers=1):\n",
        "    super().__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm_layer = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers,batch_first = True)\n",
        "    \n",
        "    self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
        "      \n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "    embedded = self.embedding(x)\n",
        "    outputs, (hidden, cell) = self.lstm_layer(embedded)\n",
        "    \n",
        "    pred = self.output_layer(hidden[-1])\n",
        "    return pred\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embedding_size = 64\n",
        "output_dim = len(np.unique(train_data['Target']))\n",
        "model = SimpleLstm(embedding_dim=embedding_size, vocab_size=vocab_size, hidden_dim=10,output_dim=output_dim).to(device).float()"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5MfaLQA1ibV"
      },
      "source": [
        "## Model summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYcxZQC-N_jF",
        "outputId": "72a2d00f-8dc8-4873-c978-5fefd203b447"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SimpleLstm(\n",
            "  (embedding): Embedding(8983, 64)\n",
            "  (lstm_layer): LSTM(64, 10, batch_first=True)\n",
            "  (output_layer): Linear(in_features=10, out_features=5, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tm2WeGqSy1l"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3vNEhD9R9fL"
      },
      "source": [
        "#TODO: Implement Model train function which will return epoch loss and accuracy\n",
        "\n",
        "def train(model, data_loader, optimizer, criterion, device):\n",
        "  epoch_loss = 0\n",
        "  epoch_accuracy = 0  \n",
        "  model.train()\n",
        "  model.to(device)\n",
        "  for batch in data_loader:\n",
        "    text = batch[0]\n",
        "    label = batch[1]\n",
        "    text, label = text.to(device), label.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(text).squeeze(1)\n",
        "    loss = criterion(predictions, label.float())\n",
        "\n",
        "    acc = accuracy_calculator(predictions, label.float())\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    epoch_loss += loss.item()\n",
        "    epoch_accuracy += acc\n",
        "\n",
        "  return epoch_loss / len(data_loader) , epoch_accuracy / len(data_loader) "
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk-6Ymn7twdi"
      },
      "source": [
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        " \n",
        "def accuracy_calculator(preds, y):\n",
        "      success = 0\n",
        "      all = preds.shape[0]\n",
        "      for i in range(preds.shape[0]):\n",
        "        index_max_predicted = np.argmax((F.softmax(((preds[i].cpu())), dim=0)).detach().numpy()) \n",
        "        index_max_label = np.argmax((y[i].cpu()).detach().numpy()) \n",
        "        if (index_max_label==index_max_predicted):\n",
        "          success+=1\n",
        "      \"\"\"Returns accuracy per batch\"\"\"\n",
        "      return success/all"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6oGin2PUTre",
        "outputId": "04157107-8838-47c9-d173-b4017b45a9f0"
      },
      "source": [
        "# Train loop \n",
        "criterion = torch.nn.BCEWithLogitsLoss() \n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "criterion = criterion.to(device)\n",
        "n_epochs = 4\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  train_loss, train_acc = train(model, loader, optimizer, criterion, device=device)\n",
        "  print(f\"Loss : {train_loss}\")\n",
        "  print(f\"Acc : {train_acc}\")"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss : 0.5045251957790935\n",
            "Acc : 0.20555936856554563\n",
            "Loss : 0.5027887248878374\n",
            "Acc : 0.2336993822923816\n",
            "Loss : 0.502816324579282\n",
            "Acc : 0.21551132463967054\n",
            "Loss : 0.5033206155468686\n",
            "Acc : 0.23164035689773507\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9CTu5oI-XfH"
      },
      "source": [
        "## Transformers & Bert\n",
        "\n",
        "Sentiment analysis task. We are going to use the [transformers library](https://github.com/huggingface/transformers) to get pre-trained transformers and use them as embedding layers. Its possible to implement from scratch. Bert is one of the popular transformer based models, <br>\n",
        "* **Name other transformer based state-of-the-art models** <br>\n",
        "\n",
        "The transformers library can be easily installed with pip <br>`pip install transformers`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0VHgjie-W6T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a7dbdc7-79d9-4b5b-d586-91dfc32ffa57"
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "init_token = tokenizer.cls_token\n",
        "eos_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "\n",
        "print(init_token, eos_token, pad_token, unk_token)\n",
        "\n",
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.44)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "[CLS] [SEP] [PAD] [UNK]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz67tzqbYNtM"
      },
      "source": [
        "def tokenize_and_trim(sentence):\n",
        "  tokens = tokenizer.tokenize(sentence) \n",
        "  tokens = tokens[:max_input_length-2]\n",
        "  return tokens"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRfq6igNYahn"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BertBasedSentiment(nn.Module):\n",
        "  def __init__(self, transform_based_model, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
        "    super().__init__()\n",
        "    self.transform_based_model = transform_based_model\n",
        "\n",
        "    self.embedding_dim = transform_based_model.config.to_dict()['hidden_size']\n",
        "    self.gru = nn.GRU(self.embedding_dim, hidden_dim, num_layers = n_layers,bidirectional = bidirectional,batch_first = True,dropout = 0 if n_layers < 2 else dropout)\n",
        "    self.output_layer = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "  def forward(self, text):\n",
        "    # First pass the input text through bert. The output of bert is like embedding Remember: Bert is set to not trainable mode\n",
        "    with torch.no_grad():\n",
        "      embed = self.transform_based_model(text)[0]\n",
        "      \n",
        "    _, hidden = self.gru(embed)\n",
        "\n",
        "    if self.gru.bidirectional:\n",
        "      hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "    else:\n",
        "      hidden = self.dropout(hidden[-1,:,:])\n",
        "      \n",
        "    return self.output_layer(hidden)\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMAuEWLSYyCa"
      },
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "hidden_dim = 256\n",
        "out_dim = len(np.unique(train_data['Target']))\n",
        "bi_directional = True\n",
        "dropout_rate = 0.25\n",
        "n_layers = 2\n",
        "\n",
        "model = BertBasedSentiment(bert_model, hidden_dim, out_dim, n_layers, bi_directional, dropout_rate).to(device).float()"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrS-EtbZH4R"
      },
      "source": [
        "#Set all the bert weights non-trainable\n",
        "for name, param in model.named_parameters():\n",
        "  if name.startswith('bert'):\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_vfQqElZNyh"
      },
      "source": [
        "## Train Bert model\n",
        "\n",
        "First, we will specify the algorithm to update the model we parameters in the training process - optimizer. The most common is stochastic gradient descent (SGD). Secondly, we will specify the loss calculation function which is selected based on the learning objective (regression, classification, ..). Lastly, all the specified algorithms must be placed in the same training device where the model is"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bzdNyzlZQkC"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# TODO: Select the optimizer and loss function/criterion\n",
        "#FIRST\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# TODO: Select the optimizer and loss function/criterion\n",
        "#SECOND\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngRq4wJYZpny",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c497d46-a1eb-43d1-b20f-2ce612af2ed5"
      },
      "source": [
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_loss, train_acc = train(model, loader, optimizer, criterion, device)\n",
        "  print(f\"Epoch: {epoch}, Loss :{train_loss}, Accuracy: {train_acc}\")"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss :0.5034722391267958, Accuracy: 0.213452299245024\n",
            "Epoch: 1, Loss :0.5015749610812188, Accuracy: 0.21859986273164037\n",
            "Epoch: 2, Loss :0.5027012134670148, Accuracy: 0.22203157172271792\n",
            "Epoch: 3, Loss :0.5014712463197597, Accuracy: 0.2361015785861359\n",
            "Epoch: 4, Loss :0.5006756878054625, Accuracy: 0.24056280027453672\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j29kvtTFeYk"
      },
      "source": [
        "## Lab Task \n",
        "\n",
        "```\n",
        "1. Write a predict function that takes in a trained net, a plain tweet text and prints out a tweet topic (label).\n",
        "2. Add make bi-directional LSTM for the classification of tweet topic. (Modify the simple LSTM model example)\n",
        "3. Create a validation set from the training data and log the models loss (training and validation) on tensorboard\n",
        "4. Visialize simple LSTM and transformer based model (bert) perfomence using confussion matrix \n",
        "\n",
        "```\n",
        "\n",
        "<center>Don't forget to commit</center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13wZnZ6cidsx"
      },
      "source": [
        "#1. Write a predict function that takes in a trained net, a plain tweet text and prints out a tweet topic (label).\n",
        "\n",
        "def predict_topic(model, text):\n",
        "  clear_text = \n",
        "  return model.predict(clear_text)\n",
        "\n",
        "text = \"some text\"\n",
        "print(predict_topic(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_1ID-W6ijjO"
      },
      "source": [
        "#2. Add make bi-directional LSTM for the classification of tweet topic. (Modify the simple LSTM model example)\n",
        "\n",
        "class SimpleLstm(nn.Module):\n",
        "  def __init__(self, embedding_dim ,vocab_size , hidden_dim=10, output_dim=1, n_layers=1):\n",
        "    super().__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm_layer = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers,batch_first = True, bidirectional=True)\n",
        "    \n",
        "    self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
        "      \n",
        "  def forward(self, x):\n",
        "    batch_size = x.size(0)\n",
        "    embedded = self.embedding(x)\n",
        "    outputs, (hidden, cell) = self.lstm_layer(embedded)\n",
        "    \n",
        "    pred = self.output_layer(hidden[-1])\n",
        "    return pred\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embedding_size = 64\n",
        "output_dim = len(np.unique(train_data['Target']))\n",
        "model = SimpleLstm(embedding_dim=embedding_size, vocab_size=vocab_size, hidden_dim=10,output_dim=output_dim).to(device).float()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssPPm1OGimAq"
      },
      "source": [
        "#3. Create a validation set from the training data and log the models loss (training and validation) on tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-L9mw2NiqYP"
      },
      "source": [
        "#4. Visialize simple LSTM and transformer based model (bert) perfomence using confussion matrix "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSUY6VFQs5I9"
      },
      "source": [
        "## References\n",
        "1. [Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](http://www.kurious.pub/blog/Illustrated-Guide-to-LSTMs-and-GRUs-A-step-by-step-explanation-6)\n",
        "\n",
        "2. [BERT Explained: What You Need to Know About Google’s New Algorithm](https://www.searchenginejournal.com/bert-explained-what-you-need-to-know-about-googles-new-algorithm/337247/#close)\n",
        "\n",
        "3. [Understanding searches better than ever before](https://www.blog.google/products/search/search-language-understanding-bert/)\n",
        "\n",
        "4. [BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)\n",
        "\n",
        "5. [How do Transformers Work in NLP? A Guide to the Latest State-of-the-Art Models](https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/)\n",
        "\n",
        "6. [A deep dive into BERT: How BERT launched a rocket into natural language understanding](https://searchengineland.com/a-deep-dive-into-bert-how-bert-launched-a-rocket-into-natural-language-understanding-324522)\n",
        "\n",
        "7. [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)\n",
        "\n",
        "8. [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
        "\n",
        "9. [BERT Word Embeddings Tutorial](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/)\n",
        "\n",
        "10. [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/pdf/1801.06146.pdf)\n",
        "\n",
        "11. [Efficient multi-lingual language model fine-tuning](https://nlp.fast.ai/)\n",
        "\n",
        "12. [BERT Text Classification in 3 Lines of Code Using Keras](https://towardsdatascience.com/bert-text-classification-in-3-lines-of-code-using-keras-264db7e7a358)\n",
        "\n",
        "13. [QUASI-RECURRENT NEURAL NETWORKS](https://arxiv.org/pdf/1611.01576.pdf)\n",
        "\n"
      ]
    }
  ]
}